{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from scipy.stats import zscore\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon API setup\n",
    "api_key = ''\n",
    "base_url = 'https://api.polygon.io/v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_symbols():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    sp500_symbols = pd.read_html(url, header=0)[0]['Symbol'].tolist()\n",
    "    return sp500_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = get_sp500_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dow30_symbols():\n",
    "    url = \"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\"\n",
    "    dow30_symbols = pd.read_html(url, header=0)[1]['Symbol'].tolist()\n",
    "    return dow30_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "symbols = get_dow30_symbols()\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_historical_sentiment(ticker, api_key, page=10):\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/historical/social-sentiment?symbol={ticker}&apikey={api_key}&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Convert the data to a pandas DataFrame for easier handling\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "api_key = \"\"  # Replace with your FMP API key\n",
    "ticker = \"NVDA\"\n",
    "df = get_historical_sentiment(ticker, api_key)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stocktwitsSentiment'].plot(figsize=(15, 5), title=f\"StockTwits sentiment for {ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to fetch data for a single ticker\n",
    "def fetch_data_for_ticker(ticker, start_date, end_date, df):\n",
    "    attempts = 0\n",
    "    start = start_date\n",
    "    while start < end_date:\n",
    "        try:\n",
    "            start_str = start.strftime('%Y-%m-%d')\n",
    "            end_str = end_date.strftime('%Y-%m-%d')\n",
    "            print(f\"Fetching {ticker} data from {start_str} to {end_str}\")\n",
    "\n",
    "            response = requests.get(f\"{base_url}/aggs/ticker/{ticker}/range/1/day/{start_str}/{end_str}?apiKey={api_key}\")\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()['results']\n",
    "            for day in data:\n",
    "                date = datetime.fromtimestamp(day['t'] / 1000)\n",
    "                df.loc[(ticker, date), 'volume'] = day['v']\n",
    "                df.loc[(ticker, date), 'price'] = day['c']\n",
    "\n",
    "            start += timedelta(days=1830)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            attempts += 1\n",
    "            if attempts >= 2:\n",
    "                print(f\"Skipping {ticker} after 2 failed attempts.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time period\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=15*365)\n",
    "\n",
    "# Create an empty DataFrame with MultiIndex\n",
    "columns = ['volume', 'price']\n",
    "index = pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['ticker', 'timestamp'])\n",
    "df = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "# Define a queue for thread-safe data collection\n",
    "results_queue = Queue()\n",
    "\n",
    "# Use ThreadPoolExecutor to fetch data in parallel\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.map(lambda ticker: fetch_data_for_ticker(ticker, start_date, end_date, df), symbols)\n",
    "\n",
    "# Process results from the queue and update the DataFrame\n",
    "while not results_queue.empty():\n",
    "    ticker, date, volume, price = results_queue.get()\n",
    "    df.loc[(ticker, date), 'volume'] = volume\n",
    "    df.loc[(ticker, date), 'price'] = price\n",
    "    \n",
    "# Save the DataFrame\n",
    "df.to_csv('historical_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for symbol in df.index.get_level_values(0).unique():\n",
    "    count += 1\n",
    "    df.loc[symbol, 'price'].plot()\n",
    "    print('Symbol:', symbol)\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change path to be in alpaca_rl_trader for linux\n",
    "#os.chdir('/home/joe/Python/tradingbot/gym/alpaca_rl_trader')\n",
    "\n",
    "# change path to be in alpaca_rl_trader for windows\n",
    "os.chdir('C:/Users/JoeBa/Documents/python/alpaca_rl_trader/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from fracdiff import fdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loader import load_data, SplitOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = ['MMM', 'AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'GS', 'HD', 'HON',\n",
    "#            'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PG', 'CRM', 'TRV', 'UNH', 'VZ', 'V', 'WMT']\n",
    "\n",
    "# Get test data\n",
    "data = load_data(\n",
    "    ratio=0.8, split_option=SplitOption.NO_SPLIT, symbols=[], table_name='crypto_data_hourly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X:ETHUSD'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot price_data matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for symbol in data.keys():\n",
    "    plt.figure()\n",
    "    plt.plot(data[symbol]['f_vmar'])\n",
    "    plt.title(symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check features to be unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming data_dict is your existing dictionary\n",
    "# Reconstruct each ticker's DataFrame\n",
    "reconstructed_dfs = []\n",
    "for ticker, col_data in data.items():\n",
    "    ticker_df = pd.DataFrame(col_data)\n",
    "    ticker_df['ticker'] = ticker  # Add ticker as a column for later MultiIndex creation\n",
    "    reconstructed_dfs.append(ticker_df)\n",
    "\n",
    "# Concatenate all reconstructed DataFrames\n",
    "concatenated_df = pd.concat(reconstructed_dfs)\n",
    "\n",
    "# Set the index to be a MultiIndex of ticker and the original index\n",
    "multiindex_df = concatenated_df.set_index(['ticker', concatenated_df.index])\n",
    "\n",
    "# Optional: If the original index was a timestamp, you might need to sort by the MultiIndex\n",
    "multiindex_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter columns that start with 'f_'\n",
    "f_columns = [col for col in multiindex_df.columns if col.startswith('f_')]\n",
    "\n",
    "# Calculate the correlation matrix for only those columns\n",
    "correlation_matrix = multiindex_df[f_columns].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 12))  # Set the size of the figure\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for high correlation\n",
    "threshold = 0.8\n",
    "\n",
    "# Find and collect highly correlated pairs\n",
    "to_drop = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            # Get the name of the feature to drop (you could also use other logic to choose which one to drop)\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            to_drop.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what features need to be dropped\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action_vector(action_vector):\n",
    "    # output is tanh, we want deadzone safespace between -0.7 and -1\n",
    "    # Scale values from [-0.7, 1] to [0, 1]\n",
    "    min_value = -0.7\n",
    "    max_value = 1\n",
    "\n",
    "    return np.clip((action_vector - min_value) / (max_value - min_value), 0,1)\n",
    "\n",
    "def bin_action_values(action_vector):\n",
    "    # Define the bin edges\n",
    "    # max is never 1 to dissallow 100% utilization of the portfolio\n",
    "    bins = np.arange(0, 1.0, 0.025)\n",
    "    action_vector_reshaped = action_vector.reshape(-1, 1)\n",
    "    abs_diff = np.abs(action_vector_reshaped - bins)\n",
    "    closest_bin_indices = np.argmin(abs_diff, axis=1)\n",
    "    binned_action_vector = bins[closest_bin_indices]\n",
    "\n",
    "    return binned_action_vector\n",
    "\n",
    "def normalize_action_vector(action_vector):\n",
    "    # scale sum of action vector to 1\n",
    "    action_vector_sum = np.sum(action_vector)\n",
    "    action_vector = action_vector / action_vector_sum\n",
    "\n",
    "    return action_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "#actions_vector = np.array([-1, -1, -1, -1, 0, 0.1, 1])  # Sample vector\n",
    "actions_vector = np.array([-1, -0.8, -0.7, -0.5, 0, 0.5, 1])  # Sample vector\n",
    "actions_vector = scale_action_vector(actions_vector)\n",
    "scaled_binned_actions = normalize_action_vector(actions_vector)\n",
    "bin_binned_actions = bin_action_values(scaled_binned_actions)\n",
    "print(bin_binned_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_binned_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_binned_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class TradingBot:\n",
    "    def __init__(self, decay_factor=0.9, frame_stack=10):\n",
    "        self.decay_factor = decay_factor  # Decay factor for exponential sampling\n",
    "        self.frame_stack = frame_stack  # Number of frames to stack\n",
    "        self.observations = []  # Store observations with decay\n",
    "        # Other initialization code...\n",
    "        self.features = ['f_percentage_change_zscore', 'f_dollar_volume_zscore', 'f_sma',\n",
    "            'f_fractional_difference_price', 'f_vmar', 'f_cumulative_return']\n",
    "\n",
    "    def next_observation(self, index=None):\n",
    "        \n",
    "        feature_arrays = []\n",
    "        \n",
    "        # Dynamic extraction of features\n",
    "        for feature_name in self.features:\n",
    "            feature_array = np.array([data[symbol][feature_name][index]\n",
    "                                      for symbol in data.keys()], dtype=np.float32)\n",
    "            feature_arrays.append(feature_array)\n",
    "\n",
    "        # Perform the division with safe check\n",
    "        holdings_array = np.zeros( len(data.keys())  )\n",
    "\n",
    "        # Ensure the result is in float32 format\n",
    "        holdings_array = holdings_array.astype(np.float32)\n",
    "\n",
    "        # Calculate unrealized P/L\n",
    "        unrealized_pl_array = np.zeros( len(data.keys())  )\n",
    "\n",
    "        # Ensure the result is in float32 format\n",
    "        unrealized_pl_array = unrealized_pl_array.astype(np.float32)\n",
    "\n",
    "        # Construct the observation array for the current step\n",
    "        # Shape will be [num_features, num_symbols]\n",
    "        current_observation = np.stack(\n",
    "            [unrealized_pl_array, holdings_array] + feature_arrays, axis=0)\n",
    "\n",
    "        # Transpose the observation to make it [num_symbols, num_features]\n",
    "        current_observation = np.transpose(\n",
    "            np.clip(current_observation, -3, 3), (1, 0))\n",
    "\n",
    "        # Apply exponential decay to historical observations\n",
    "        self.observations = [obs * self.decay_factor for obs in self.observations]\n",
    "\n",
    "        # Add the current observation\n",
    "        self.observations.append(current_observation)\n",
    "\n",
    "        # Ensure the deque has a maximum length of FRAMESTACK\n",
    "        self.observations = self.observations[-self.frame_stack:]\n",
    "\n",
    "        # Stack the observations to create the final array\n",
    "        # This will stack the last FRAMESTACK observations along a new dimension\n",
    "        stacked_obs = np.stack(self.observations, axis=-1)\n",
    "\n",
    "\n",
    "        return stacked_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TradingBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x\n",
    "x = tb.next_observation(index=212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot x\n",
    "count+=1\n",
    "x = tb.next_observation(index=count)\n",
    "# Assuming 'x' is the output with shape (15, 8, 10)\n",
    "# Choose a symbol to visualize, for example, symbol 0\n",
    "symbol_index = 0\n",
    "\n",
    "num_features = x.shape[1]\n",
    "num_frames = x.shape[2]\n",
    "\n",
    "# Create a figure with subplots for each feature\n",
    "fig, axs = plt.subplots(num_features, 1, figsize=(10, 20))\n",
    "\n",
    "# Iterate over each feature\n",
    "for i in range(num_features):\n",
    "    # Plot the feature across all frames for the chosen symbol\n",
    "    axs[i].plot(range(num_frames), x[symbol_index, i, :], marker='o')\n",
    "    axs[i].set_title(f'Feature {i+1} over Time for Symbol {symbol_index+1}')\n",
    "    axs[i].set_xlabel('Frame')\n",
    "    axs[i].set_ylabel('Feature Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
